{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c268d6f8-7757-49d0-be33-8047b84bc1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0314b-6a69-44e4-8421-5c25163cded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download, hf_hub_download\n",
    "from datasets import Dataset\n",
    "\n",
    "REXVQA_REPO = \"rajpurkarlab/ReXVQA\"\n",
    "REXGRAD_REPO = \"rajpurkarlab/ReXGradient-160K\"\n",
    "\n",
    "\n",
    "meta_path = snapshot_download(repo_id=REXGRAD_REPO, repo_type=\"dataset\")\n",
    "\n",
    "!cat {meta_path}/deid_png.part* > deid_png.tar\n",
    "!tar -xf /home/deid_png.tar\n",
    "meta_path = snapshot_download(repo_id=REXVQA_REPO, repo_type=\"dataset\")\n",
    "!cp  {meta_path}/metadata/test_vqa_data.json  /home/QA_json/\n",
    "!cp  {meta_path}/metadata/train_vqa_data.json  /home/QA_json/\n",
    "!cp  {meta_path}/metadata/valid_vqa_data.json  /home/QA_json/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96303e86-2c85-497d-ab09-502f14323750",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configuration file for MedGemma SFT training\n",
    "\"\"\"\n",
    "\n",
    "# Dataset paths - Windows paths\n",
    "TRAIN_JSON = \"/home/QA_json/train_vqa_data.json\"\n",
    "VAL_JSON = \"/home/QA_json/valid_vqa_data.json\"\n",
    "TEST_JSON = \"/home/QA_json/test_vqa_data.json\"\n",
    "\n",
    "# Model configuration\n",
    "MODEL_ID = \"unsloth/medgemma-4b-it\"  # Medical vision-language model\n",
    "USE_ONLY_FIRST_IMAGE = False  # Set to True to use only the first image per sample\n",
    "\n",
    "# Training configuration - SFT with Unsloth\n",
    "TRAINING_CONFIG = {\n",
    "    \"output_dir\": \"medgemma4b_it_sft_reasoning\",\n",
    "    \"per_device_train_batch_size\": 2,  # Batch size\n",
    "    \"gradient_accumulation_steps\": 4,  # Effective batch size = 2 * 4 = 8\n",
    "    \"learning_rate\": 2e-4,  # Higher learning rate for SFT\n",
    "    \"num_train_epochs\": 3,  # More epochs for SFT\n",
    "    \"max_steps\": -1,  # Use epochs instead of max_steps\n",
    "    \"max_seq_length\": 2048,  # Maximum sequence length\n",
    "    \"bf16\": True,\n",
    "    \"remove_unused_columns\": False,\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_steps\": 500,\n",
    "    \"save_total_limit\": 3,\n",
    "    \"report_to\": \"none\",\n",
    "    \"dataloader_num_workers\": 0,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"warmup_steps\": 50,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "}\n",
    "# LoRA configuration\n",
    "LORA_CONFIG = {\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    \"task_type\": \"CAUSAL_LM\"\n",
    "}\n",
    "\n",
    "# Quantization configuration\n",
    "QUANTIZATION_CONFIG = {\n",
    "    \"load_in_4bit\": True,\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "    \"bnb_4bit_use_double_quant\": True,\n",
    "    \"bnb_4bit_compute_dtype\": \"bfloat16\"\n",
    "}\n",
    "\n",
    "# Response format tokens\n",
    "REASONING_START = \"<start_working_out>\"\n",
    "REASONING_END = \"<end_working_out>\"\n",
    "SOLUTION_START = \"<SOLUTION>\"\n",
    "SOLUTION_END = \"</SOLUTION>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c56612b-860c-4330-a0ab-912be93a9589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Patching Xformers to fix some performance issues.\n",
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Torch: 2.5.0+cu124\n",
      "CUDA available: True\n",
      "Loading model: unsloth/medgemma-4b-it\n",
      "==((====))==  Unsloth 2025.8.5: Fast Gemma3 patching. Transformers: 4.55.2.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.209 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.0+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `base_model.model.model.vision_tower.vision_model` require gradients\n",
      "Model loaded successfully with LoRA adapters\n",
      "Creating iterable training dataset...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastVisionModel\n",
    "from transformers import TextStreamer\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "import json\n",
    "from datasets import Dataset, IterableDataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Load model and tokenizer using Unsloth\n",
    "print(f\"Loading model: {MODEL_ID}\")\n",
    "\n",
    "\n",
    "model, processor = FastVisionModel.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    ")\n",
    "# Add LoRA adapters\n",
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
    "    finetune_language_layers   = True, # False if not finetuning language layers\n",
    "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
    "\n",
    "    r = 16,                           # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha = 16,                  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,               # We support rank stabilized LoRA\n",
    "    loftq_config = None,               # And LoftQ\n",
    "    target_modules = \"all-linear\",    # Optional now! Can specify a list if needed\n",
    "    modules_to_save=[\n",
    "        \"lm_head\",\n",
    "        \"embed_tokens\",\n",
    "    ],\n",
    ")\n",
    "print(\"Model loaded successfully with LoRA adapters\")\n",
    "\n",
    "import os, json\n",
    "\n",
    "def create_iterable_dataset_generator(json_path, images_base_path=\".\"):\n",
    "    \"\"\"Create a generator for iterable dataset with role/content format.\"\"\"\n",
    "    def generator():\n",
    "        print(f\"Loading dataset from {json_path}\")\n",
    "\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        processed = 0\n",
    "\n",
    "        # Handle both dict and list formats\n",
    "        items = data.values() if isinstance(data, dict) else data\n",
    "\n",
    "        for item in items:\n",
    "            # Resolve image path\n",
    "            image_path = None\n",
    "            if 'image_path' in item:\n",
    "                raw_path = item['image_path']\n",
    "                if isinstance(raw_path, list):\n",
    "                    raw_path = raw_path[0]\n",
    "                if isinstance(raw_path, str) and raw_path.startswith('../'):\n",
    "                    # Map ../... to /home/... (adjust if needed)\n",
    "                    clean_path = raw_path.replace('../', '/home/')\n",
    "                    image_path = os.path.normpath(clean_path)\n",
    "                else:\n",
    "                    image_path = os.path.join(images_base_path, raw_path) if isinstance(raw_path, str) else None\n",
    "\n",
    "            elif 'ImagePath' in item and item['ImagePath']:\n",
    "                image_paths = item['ImagePath']\n",
    "                raw_path = image_paths[0] if isinstance(image_paths, list) and image_paths else image_paths\n",
    "                if isinstance(raw_path, str) and raw_path.startswith('../'):\n",
    "                    clean_path = raw_path.replace('../', '/home/')\n",
    "                    image_path = os.path.normpath(clean_path)\n",
    "                else:\n",
    "                    image_path = raw_path if isinstance(raw_path, str) else None\n",
    "\n",
    "            # Skip if no image path or image doesn't exist\n",
    "            if not image_path or not os.path.exists(image_path):\n",
    "                continue\n",
    "\n",
    "            # Build instruction/question and assistant response\n",
    "            instruction = item.get('question', '')\n",
    "\n",
    "            reasoning = item.get('heur_reason', item.get('reason', ''))\n",
    "            answer_letter = item.get('answer', item.get('correct_answer', ''))\n",
    "            explanation = item.get('explanation', item.get('correct_answer_explanation', ''))\n",
    "\n",
    "            assistant_response = (\n",
    "                f\"{REASONING_START}\\n{reasoning}\\n{REASONING_END}\\n\\n\"\n",
    "                f\"{SOLUTION_START}\\n{answer_letter}: {explanation}\\n{SOLUTION_END}\"\n",
    "            )\n",
    "            processed += 1\n",
    "            if processed % 1000 == 0:\n",
    "                print(f\"Processed {processed} samples...\")\n",
    "\n",
    "            # Yield in the requested role/content format\n",
    "            yield [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": instruction},\n",
    "                        {\"type\": \"image\", \"image\": Image.open(image_path).convert('RGB')},\n",
    "                    ],\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": assistant_response},\n",
    "                    ],\n",
    "                },\n",
    "            ]\n",
    "\n",
    "    \n",
    "    return generator\n",
    "\n",
    "\n",
    "print(\"Creating iterable training dataset...\")\n",
    "train_generator = create_iterable_dataset_generator(TRAIN_JSON)\n",
    "train_dataset = IterableDataset.from_generator(train_generator)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1830984-c791-46de-8764-e2586e251d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from /home/QA_json/train_vqa_data.json\n"
     ]
    }
   ],
   "source": [
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "FastVisionModel.for_training(model) # Enable for training!\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    processing_class=processor.tokenizer,\n",
    "    data_collator=UnslothVisionDataCollator(model, processor),\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        gradient_checkpointing = True,\n",
    "\n",
    "        # use reentrant checkpointing\n",
    "        gradient_checkpointing_kwargs = {\"use_reentrant\": False},\n",
    "        max_grad_norm = 0.3,              # max gradient norm based on QLoRA paper\n",
    "        warmup_ratio = 0.03,\n",
    "        max_steps = 30,\n",
    "        #num_train_epochs = 2,          # Set this instead of max_steps for full training runs\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        save_strategy=\"steps\",\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",             # For Weights and Biases\n",
    "\n",
    "        # You MUST put the below items for vision finetuning:\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        max_length = 2048,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Starting SFT training...\")\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "model.save_pretrained(TRAINING_CONFIG[\"output_dir\"])\n",
    "tokenizer.save_pretrained(TRAINING_CONFIG[\"output_dir\"])\n",
    "\n",
    "# Save to GGUF format for inference (optional)\n",
    "print(\"Saving model in GGUF format...\")\n",
    "model.save_pretrained_gguf(\n",
    "    f\"{TRAINING_CONFIG['output_dir']}_gguf\", \n",
    "    tokenizer, \n",
    "    quantization_method=\"q4_k_m\"\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571c746b-f07a-42a8-8939-9d5a38199586",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f64181-521f-467b-af3e-76ac51d0a8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
